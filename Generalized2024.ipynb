{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9146e3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib ipympl \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import multivariate_normal\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import boxcox\n",
    "from scipy.stats import yeojohnson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9a95b980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all data into dataframe\n",
    "def load_data(path, file_names, aliases):\n",
    "    dates = {}\n",
    "    for data_set_idx in range(len(data_files)):\n",
    "        cur_alias = aliases[data_set_idx]\n",
    "        with open(path + data_files[data_set_idx] + '.csv', newline='') as csvfile:\n",
    "            spamreader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "            spamreader.__next__()\n",
    "            for row in spamreader:\n",
    "                try:\n",
    "                    cur_date = datetime.datetime.strptime(row[0], '%m/%d/%Y')\n",
    "\n",
    "                except Exception as e: \n",
    "                   continue\n",
    "                if not cur_date in dates:\n",
    "                    dates[cur_date] = {}\n",
    "#                     # need to generalize here\n",
    "#                 if data_set_idx == 0 or data_set_idx == 2:\n",
    "#                     dates[cur_date][cur_alias] = float(row[4])\n",
    "#                 elif data_set_idx == 1 or data_set_idx == 3 or data_set_idx == 4 or data_set_idx == 5:\n",
    "                try:\n",
    "                    dates[cur_date][cur_alias] = float(row[1])\n",
    "                except:\n",
    "                    print(row[1])\n",
    "                    print(cur_alias)\n",
    "                    print(row)\n",
    "                    \n",
    "\n",
    "    df = pd.DataFrame.from_dict(dates, orient='index')\n",
    "    # df.columns = aliases\n",
    "    df.reset_index(inplace=True)\n",
    "    df = df.rename(columns = {'index':'Date'})\n",
    "    df = df.sort_values('Date')\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b1191f6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# not_null = df.query(baseline_asset + \".notnull()\")\n",
    "# not_null.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d9cc06b2-3eea-4c50-990a-43c93913f4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_div_data(asset_list, file_list):\n",
    "    div_data = {}\n",
    "    for asset, file_path in zip(asset_list, file_list):\n",
    "        # Initialize data structure for the asset\n",
    "        div_data[asset] = {\"payment_date\": set(), \"ex_date\": set(), \"amount\": {}}\n",
    "        \n",
    "        # Read CSV file into DataFrame\n",
    "        if file_path != None:\n",
    "            df = pd.read_csv(file_path, delimiter=',', header=0)\n",
    "            \n",
    "            # Iterate over rows in the DataFrame\n",
    "            for index, row in df.iterrows():\n",
    "                # Extract relevant data\n",
    "                ex_date = row[\"Ex/EFF Date\"]\n",
    "                cash_amount = row[\"Cash Amount\"]\n",
    "                payment_date = row[\"Payment Date\"]\n",
    "                \n",
    "                # Update div_data with extracted data\n",
    "                div_data[asset][\"ex_date\"].add(ex_date)\n",
    "                div_data[asset][\"payment_date\"].add(payment_date)\n",
    "                div_data[asset][\"amount\"][ex_date] = cash_amount\n",
    "            \n",
    "    return div_data\n",
    "\n",
    "\n",
    "def get_x_days_ret(asset, df, div_data, distance, idx):\n",
    "    start_idx = 0\n",
    "    end_idx = 0\n",
    "    if distance < 0:\n",
    "        distance = abs(distance)\n",
    "        start_idx = idx - distance\n",
    "        end_idx = idx + 1\n",
    "    else:\n",
    "        start_idx = idx\n",
    "        end_idx = idx + distance + 1\n",
    "        \n",
    "    num_shares = 1\n",
    "    dollars = 0\n",
    "    for i in range(start_idx, end_idx):\n",
    "        if df.iloc[i][\"Date\"] in div_data[\"payment_date\"]:\n",
    "            num_shares += dollars / df.iloc[i][asset]\n",
    "            dollars = 0\n",
    "        if df.iloc[i][\"Date\"] in div_data[\"ex_date\"]:\n",
    "            dollars += div_data[\"amount\"][df.iloc[i][\"Date\"]] * num_shares\n",
    "    final_val = num_shares * df.iloc[end_idx - 1][asset] + dollars\n",
    "    start_val = df.iloc[start_idx][asset]\n",
    "    return (final_val - start_val) / start_val\n",
    "            \n",
    "            \n",
    "            \n",
    "    \n",
    "\n",
    "def add_correlaries_div(cor_assets, cor_days_out, pred_distance, df, assets, div_data):\n",
    "    # stores percent changes from past x days \n",
    "    cors = [[] for i in range(len(cor_assets))]\n",
    "    # stores percent changes for x future days for each asset\n",
    "    futs = {}\n",
    "    for a in assets:\n",
    "        futs[a] = []\n",
    "    \n",
    "    # iterate through all data points\n",
    "    for idx, row in df.iterrows():\n",
    "        # past data points\n",
    "        for alias_idx, (asset, days_out) in enumerate(zip(cor_assets, cor_days_out)):\n",
    "            if idx > days_out: # check for enough data\n",
    "                # get percent change\n",
    "                time_period_change = get_x_days_ret(asset, df, div_data[asset], -days_out, idx)  \n",
    "                cors[alias_idx].append(time_period_change)\n",
    "            else:\n",
    "                cors[alias_idx].append(None)\n",
    "        \n",
    "        #future data\n",
    "        for asset in assets:\n",
    "            cur_price = row[asset]\n",
    "            if idx + pred_distance < df.shape[0] and not pd.isna(cur_price) and not pd.isna(df.iloc[idx + pred_distance][asset]):\n",
    "                time_period_change = get_x_days_ret(asset, df, div_data[asset], pred_distance, idx)  \n",
    "                futs[asset].append(time_period_change) \n",
    "            else:\n",
    "                futs[asset].append(None)\n",
    "    # input into data frame\n",
    "    for idx, (asset, days_out) in enumerate(zip(cor_assets, cor_days_out)):\n",
    "        name = asset + \"_\" + str(days_out) + \"_dys\"\n",
    "        df.insert(df.shape[1], name, cors[idx], True)\n",
    "    \n",
    "    for asset in futs.keys():\n",
    "        name = asset + \"_fut_\" + str(pred_distance) + \"dys\"\n",
    "        df.insert(df.shape[1], name, futs[asset], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1e04a59e-595b-4dfb-b979-b9b6adcd56d3",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'IYRDividends.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m div_data \u001b[38;5;241m=\u001b[39m load_div_data([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mre\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIYRDividends.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "Cell \u001b[0;32mIn[45], line 9\u001b[0m, in \u001b[0;36mload_div_data\u001b[0;34m(asset_list, file_list)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Read CSV file into DataFrame\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file_path \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m----> 9\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(file_path, delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# Iterate over rows in the DataFrame\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m index, row \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;66;03m# Extract relevant data\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ProbStock/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ProbStock/lib/python3.11/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ProbStock/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ProbStock/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1881\u001b[0m     f,\n\u001b[1;32m   1882\u001b[0m     mode,\n\u001b[1;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1889\u001b[0m )\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ProbStock/lib/python3.11/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'IYRDividends.csv'"
     ]
    }
   ],
   "source": [
    "div_data = load_div_data([\"re\"], [\"IYRDividends.csv\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9cca94d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adds correlation metrix to dataframe\n",
    "def add_correlaries(cor_assets, cor_days_out, pred_distance, df, assets):\n",
    "    # stores percent changes from past x days \n",
    "    cors = [[] for i in range(len(cor_assets))]\n",
    "    # stores percent changes for x future days for each asset\n",
    "    futs = {}\n",
    "    for a in assets:\n",
    "        futs[a] = []\n",
    "    \n",
    "    # iterate through all data points\n",
    "    for idx, row in df.iterrows():\n",
    "        # past data points\n",
    "        for alias_idx, (asset, days_out) in enumerate(zip(cor_assets, cor_days_out)):\n",
    "            cur_price = row[asset]\n",
    "            if idx > days_out: # check for enough data\n",
    "                # get percent change\n",
    "                last_time_period = df.loc[idx - days_out - 1].at[asset]\n",
    "                time_period_change = (cur_price - last_time_period)/last_time_period\n",
    "                cors[alias_idx].append(time_period_change)           \n",
    "            else:\n",
    "                cors[alias_idx].append(None)\n",
    "        \n",
    "        #future data\n",
    "        for asset in assets:\n",
    "            cur_price = row[asset]\n",
    "            if idx + pred_distance < df.shape[0] and not pd.isna(cur_price) and not pd.isna(df.iloc[idx + pred_distance].at[asset]):\n",
    "                fut_val = df.iloc[idx + pred_distance].at[asset]\n",
    "                time_period_change = (fut_val - cur_price)/cur_price  \n",
    "                futs[asset].append(time_period_change) \n",
    "            else:\n",
    "                futs[asset].append(None)\n",
    "    # input into data frame\n",
    "    for idx, (asset, days_out) in enumerate(zip(cor_assets, cor_days_out)):\n",
    "        name = asset + \"_\" + str(days_out) + \"_dys\"\n",
    "        df.insert(df.shape[1], name, cors[idx], True)\n",
    "    \n",
    "    for asset in futs.keys():\n",
    "        name = asset + \"_fut_\" + str(pred_distance) + \"dys\"\n",
    "        df.insert(df.shape[1], name, futs[asset], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "302592d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_pred_differences(pred_distance, baseline_asset, assets, df):\n",
    "    for idx, asset in enumerate(assets):\n",
    "#       for idx2, asset2 in enumerate(assets[idx + 1: ]): if you want all differences\n",
    "        if asset != baseline_asset:\n",
    "            change_asset = df[asset + \"_fut_\" + str(pred_distance) + \"dys\"]\n",
    "            change_baseline = df[baseline_asset + \"_fut_\" + str(pred_distance) + \"dys\"]\n",
    "            diff = change_asset - change_baseline \n",
    "            df.insert(df.shape[1], asset + \"_\" + baseline_asset + \"_\" + str(pred_distance) + \"dys_diff\", diff, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8c42dcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not_null.iloc[5700:5750,[0, 7,8, 9, 10,11,12,13,14,15,16,17,18,19,20]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "38b6777e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not_null.iloc[-160:,0: 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0a440989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not_null.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f6a6e141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #use sklearn.preprocessing.PowerTransformer instead\n",
    "\n",
    "# plt.figure()\n",
    "\n",
    "# column_name = 'sp_20_dys'\n",
    "# column = not_null[column_name] \n",
    "# column = column[~np.isnan(column)]\n",
    "# print(column)\n",
    "# # column += np.array([1 for i in range(len(column))])\n",
    "# # print(column)\n",
    "# plt.figure()\n",
    "# plt.hist(column , color = 'red', bins = 500, density=True)\n",
    "# mean = np.mean(column)\n",
    "# std = np.std(column)\n",
    "# print(mean)\n",
    "# print(std)\n",
    "# x_axis = np.arange(-.3, .3, 0.01)\n",
    "\n",
    "# plt.plot(x_axis, norm.pdf(x_axis, mean, std))\n",
    "\n",
    "\n",
    "# plt.figure()\n",
    "# plt.hist(yeojohnson(column)[0] , color = 'red', bins = 500, density=True)\n",
    "# mean = np.mean(yeojohnson(column)[0])\n",
    "# std = np.std(yeojohnson(column)[0])\n",
    "# print(mean)\n",
    "# print(std)\n",
    "# x_axis = np.arange(-.3, .3, 0.01)\n",
    "\n",
    "# plt.plot(x_axis, norm.pdf(x_axis, mean, std))\n",
    "# # plt.hist(np.log(sp_not_null[column_name] + np.array([1 for i in range(len(sp_not_null[column_name]))])) , color = 'red', bins = 500, density=True)\n",
    "# # mean = np.mean(np.log(sp_not_null[column_name] + np.array([1 for i in range(len(sp_not_null[column_name]))])))\n",
    "# # std = np.std(np.log(sp_not_null[column_name] + np.array([1 for i in range(len(sp_not_null[column_name]))])))\n",
    "\n",
    "\n",
    "\n",
    "# # column += np.array([1 for i in range(len(column))])\n",
    "# # column = np.log(column)\n",
    "# # plt.figure()\n",
    "# # plt.hist(yeojohnson(column)[0] , color = 'red', bins = 500, density=True)\n",
    "# # mean = np.mean(yeojohnson(column)[0])\n",
    "# # std = np.std(yeojohnson(column)[0])\n",
    "# # print(mean)\n",
    "# # print(std)\n",
    "# # x_axis = np.arange(-.3, .3, 0.01)\n",
    "\n",
    "# plt.plot(x_axis, norm.pdf(x_axis, mean, std))\n",
    "# # plt.hist(sp_not_null['sp_fut_2wks'], color = 'red', bins = 500)\n",
    "# # plt.hist(sp_not_null['re_fut_2wks'], color = 'green', bins = 500, alpha = .5,)\n",
    "# # plt.hist(sp_not_null['bnd_fut_2wks'], color = 'blue', bins = 500, alpha = .5,)\n",
    "# # plt.hist(sp_not_null['gld_fut_2wks'], color = 'yellow', bins = 500, alpha = .5,)\n",
    "# # plt.hist(sp_not_null['eu_fut_2wks'], color = 'green', bins = 500, alpha = .5,)\n",
    "# # plt.hist(sp_not_null['jp_fut_2wks'], color = 'blue', bins = 500, alpha = .5,)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "13a2a30e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fig = plt.figure()\n",
    "# ax = fig.add_subplot(projection = '3d')\n",
    "\n",
    "# ax.scatter(not_null[\"sp_last_month\"], not_null[\"re_last_month\"], not_null[\"re_sp_2wk_diff\"])\n",
    "# ax.set_xlabel('sp_last_month')\n",
    "# ax.set_ylabel('re_last_month')\n",
    "# ax.set_zlabel('re_sp_2wk_diff')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6c4b85e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rvs(baseline, df, aliases, pred_distance, print_mats=False):\n",
    "    rvs = {}\n",
    "    \n",
    "    valid_cols = []\n",
    "    for col in df.columns:\n",
    "        if not col in aliases and col != \"Date\" and not \"diff\" in col and not \"fut\" in col:\n",
    "            valid_cols.append(col)\n",
    "    valid_cols.append(None)\n",
    "    \n",
    "    for asset in aliases:\n",
    "        if asset != baseline:\n",
    "            valid_cols[-1] = (asset + \"_\" + baseline + \"_\" + str(pred_distance) + \"dys_diff\")\n",
    "\n",
    "            cov_mat = df[valid_cols]\n",
    "            cov_matrix = pd.DataFrame.cov(cov_mat)\n",
    "            cov_mat = cov_mat.cov()\n",
    "            cov_mat = cov_mat.to_numpy()\n",
    "            if print_mats:\n",
    "                print(asset)\n",
    "                print(cov_matrix)\n",
    "\n",
    "\n",
    "            # means of values\n",
    "            means = []\n",
    "            for col in valid_cols:\n",
    "                means.append(np.mean(df[col]))\n",
    "            if print_mats:\n",
    "                print(means)\n",
    "\n",
    "            rv = multivariate_normal(mean=means, cov=cov_mat, allow_singular=True)\n",
    "            rvs[asset] = rv\n",
    "    return rvs\n",
    "        \n",
    "\n",
    "\n",
    "def predict(asset, baseline, rv, inputs, get_plots=False, do_print=False, get_50_pt = False):\n",
    "    START = -.22\n",
    "    STOP = .22\n",
    "    INCREMENT = .00005\n",
    "\n",
    "    probs = []\n",
    "   \n",
    "    x = np.arange(START, STOP, INCREMENT)\n",
    "    inputs.append(None)\n",
    "    for val in x:\n",
    "        # make an array with all the current values\n",
    "        # insert past month performance\n",
    "        #\"sp\", \"re\", \"bnd\", \"eu\", \"jp\", \"gld\", future difference\n",
    "        inputs[-1] = val\n",
    "        probs.append(rv.pdf(inputs))\n",
    "\n",
    "\n",
    "    cdf = []\n",
    "    for idx in range(x.size - 1):\n",
    "        cur_prob = probs[idx]\n",
    "        next_prob = probs[idx + 1]\n",
    "        rieman_sum = min(cur_prob, next_prob) * INCREMENT\n",
    "        rieman_sum += max(cur_prob, next_prob) - min(cur_prob, next_prob) * INCREMENT / 2\n",
    "        if len(cdf) > 0:\n",
    "            cdf.append(rieman_sum + cdf[-1])\n",
    "        else:\n",
    "            cdf.append(rieman_sum)\n",
    "                                 \n",
    "    if get_plots:\n",
    "        fig1 = plt.figure()\n",
    "        ax = fig1.add_subplot(111)\n",
    "        plt.title(\"pdf\")\n",
    "        plt.xlabel(\"difference between performance of \" + asset + \" and \" + baseline)\n",
    "        plt.ylabel(\"probability\")\n",
    "        ax.plot(x, probs/cdf[-1])\n",
    "        plt.show()\n",
    "\n",
    "    for idx in range(len(cdf)):\n",
    "        cdf[idx] /= cdf[-1]\n",
    "\n",
    "    if get_plots:\n",
    "        fig2 = plt.figure()\n",
    "        ax = fig2.add_subplot(111)\n",
    "        plt.title(\"cdf\")\n",
    "        plt.xlabel(\"difference between performance of \" + asset + \" and \" + baseline)\n",
    "        plt.ylabel(\"probability\")\n",
    "        ax.plot(x[:-1], cdf)\n",
    "        plt.show() \n",
    "        \n",
    "    if get_50_pt:\n",
    "        # find 50% point\n",
    "        cur_prob = 0\n",
    "        idx = 0\n",
    "        while(cur_prob < .5):\n",
    "            cur_prob = cdf[idx]\n",
    "            idx += 1\n",
    "        fiftyfiftypt = x[idx]\n",
    "        if do_print:\n",
    "            print(\"50 50 change to be above or below\")\n",
    "            print(x[idx])\n",
    "\n",
    "\n",
    "    #find expected value\n",
    "    expected_value = 0\n",
    "    for idx in range(len(cdf)):\n",
    "        if idx == 0:\n",
    "            expected_value += cdf[idx] * x[idx]\n",
    "        else:\n",
    "            cur_prob = cdf[idx - 1]\n",
    "            next_prob = cdf[idx]\n",
    "            actual_prob = next_prob - cur_prob\n",
    "            expected_value += actual_prob * x[idx]\n",
    "                                 \n",
    "    if do_print:\n",
    "        print(\"Expected Value\")\n",
    "        print(expected_value)\n",
    "                                 \n",
    "    if get_50_pt:\n",
    "        return fiftyfiftypt, expected_value\n",
    "    else:\n",
    "        return expected_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3af092e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_preds(assets, baseline, pred_distance, df_test, df_train):\n",
    "    rvs = get_rvs(baseline, df_train, assets, pred_distance)\n",
    "    preds = {}\n",
    "    actuals = {}\n",
    "    for asset in assets:\n",
    "        if asset != baseline:\n",
    "            preds[asset] = []\n",
    "            actuals[asset] = []\n",
    "    \n",
    "    pred_columns = []\n",
    "    for col_idx, col in enumerate(df.columns):\n",
    "        if not col in aliases and col != \"Date\" and not \"diff\" in col and not \"fut\" in col:\n",
    "            pred_columns.append(col_idx)\n",
    "            \n",
    "            \n",
    "    print('start')\n",
    "    for idx, row in df_test.iterrows():\n",
    "        for asset in assets:\n",
    "            if asset != baseline:\n",
    "#                 print(asset)\n",
    "                col_name = asset + \"_\" + baseline  + \"_\" + str(pred_distance) + \"dys_diff\"\n",
    "                actual = row[col_name]\n",
    "                actuals[asset].append(actual)\n",
    "#                 print(actual)\n",
    "                columns = []\n",
    "                pred_input = df_test.iloc[idx, pred_columns]\n",
    "                if not pred_input.isnull().any():\n",
    "                    prediction = predict(asset, baseline, rvs[asset], pred_input.tolist(), get_plots=False)\n",
    "                    preds[asset].append(prediction)\n",
    "                    # print(prediction)\n",
    "                else:\n",
    "                    preds[asset].append(None)\n",
    "#                     print(None)\n",
    "#                 print(\"---------\")\n",
    "        if idx % 10== 0:\n",
    "            print(idx)\n",
    "\n",
    "    return preds, actuals            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b9560539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs here\n",
    "\n",
    "# path = 'C:\\\\Users\\\\plant\\\\\n",
    "path = ''\n",
    "\n",
    "baseline_asset = 'sp'\n",
    "\n",
    "file_SP = 'SPY'  \n",
    "div_SP = 'SPYDividend'\n",
    "file_RE = 'IYR'\n",
    "div_RE = 'IYRDividend'\n",
    "file_BND = 'isharesBondIndexSince2003'\n",
    "div_BND = 'USAggBondDividend'\n",
    "file_EU = 'USD_EURHistoricalData'\n",
    "div_EU = None\n",
    "file_JPY = 'USD_JPYHistoricalData'\n",
    "div_JP = None\n",
    "file_GLD = 'GoldFuturesHistoricalData'\n",
    "div_GLD = None\n",
    "file_MID = 'IJH'\n",
    "div_MID = 'IJHDividend'\n",
    "file_SML = 'IJR'\n",
    "div_SML = 'IJRDividend'\n",
    "file_RUT = 'IWM'\n",
    "div_RUT = 'IWMDividend'\n",
    "file_EST = 'EZU'\n",
    "div_EST = 'EZUDividend'\n",
    "file_EMR = 'EEM'\n",
    "div_EMR = 'EEMDividend'\n",
    "file_JST = 'EWJ'\n",
    "div_JST = 'EWJDividend'\n",
    "\n",
    "# data_files = [file_name_SP, file_name_RE, file_name_BND, file_name_EU, file_name_JPY, file_name_GOLD, file_name_RUT]\n",
    "# aliases = [\"sp\", \"re\", \"bnd\", \"eu\", \"jp\", \"gld\", 'rut']\n",
    "\n",
    "# # input correlaries\n",
    "# cor_assets = [\"sp\", \"re\", \"bnd\", \"eu\", \"jp\", \"gld\", 'rut', \"sp\", \"re\", \"bnd\", \"eu\", \"jp\", \"gld\", 'rut', \"sp\", \"re\", \"bnd\", \"eu\", \"jp\", \"gld\", 'rut', \"sp\", \"re\", \"bnd\", \"eu\", \"jp\", \"gld\", 'rut']\n",
    "# cor_days_out = [20, 20, 20, 20, 20, 20, 20, 10, 10, 10, 10, 10, 10, 10, 252, 252, 252, 252, 252, 252, 252, 60, 60, 60, 60, 60, 60, 60]\n",
    "# pred_distance = 10\n",
    "# assets = [\"sp\", \"re\", \"bnd\", \"eu\", \"jp\", \"gld\", 'rut']\n",
    "\n",
    "\n",
    "data_files = [file_SP, file_RE, file_BND, file_EU, file_JP, file_GLD, file_MID, file_SML, file_RUT, file_EST, file_EMR, file_JST]\n",
    "div_files = [div_SP, div_RE, div_BND, div_EU, div_JP, div_GLD, div_MID, div_SML, div_RUT, div_EST, div_EMR, div_JST]\n",
    "aliases = [\"sp\", \"re\", \"bnd\", \"eu\", \"jp\", \"gld\", \"mid\", \"sml\", \"rut\", \"est\", \"emr\", \"jst\"]\n",
    "\n",
    "# input correlaries\n",
    "# cor_assets = ['sp', \"re\", \"bnd\", \"eu\", \"jp\", \"gld\", \"sp\", \"re\", \"bnd\", \"eu\", \"jp\", \"gld\", \"sp\", \"re\", \"bnd\", \"eu\", \"jp\", \"gld\",  \"sp\", \"re\", \"bnd\", \"eu\", \"jp\", \"gld\"]\n",
    "# cor_days_out = [20,   20,    20,   20,   20,    20,   10,   10,   10, 10,   10,   10,   252,  252, 252, 252,  252,   252,    60,   60,    60,    60,    60,   60]\n",
    "cor_assets = [\"sp\", \"re\", \"bnd\", \"eu\", \"jp\", \"gld\", \"mid\", \"sml\", \"rut\", \"est\", \"emr\", \"jst\"]\n",
    "cor_days_out = [10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10]\n",
    "pred_distance = 10\n",
    "assets = [\"sp\", \"re\", \"bnd\", \"eu\", \"jp\", \"gld\", \"mid\", \"sml\", \"rut\", \"est\", \"emr\", \"jst\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3920ba1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_data(path, data_files, aliases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1e92f941",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>sp</th>\n",
       "      <th>gld</th>\n",
       "      <th>jst</th>\n",
       "      <th>eu</th>\n",
       "      <th>jp</th>\n",
       "      <th>mid</th>\n",
       "      <th>sml</th>\n",
       "      <th>rut</th>\n",
       "      <th>re</th>\n",
       "      <th>est</th>\n",
       "      <th>emr</th>\n",
       "      <th>bnd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1990-01-02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>404.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date  sp    gld  jst  eu  jp  mid  sml  rut  re  est  emr  bnd\n",
       "0 1990-01-02 NaN  404.5  NaN NaN NaN  NaN  NaN  NaN NaN  NaN  NaN  NaN"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1162178c-2a28-4ad3-a0f1-2eb5f5a107ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.query(baseline_asset + \".notnull()\")\n",
    "df.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "06653bbf",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'mid_fut_10dys'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/miniconda3/envs/ProbStock/lib/python3.11/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'mid_fut_10dys'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m div_data \u001b[38;5;241m=\u001b[39m load_div_data([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msp\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mre\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbnd\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meu\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjp\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgld\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIYRDividend.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m])\n\u001b[1;32m      3\u001b[0m add_correlaries_div([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msp\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mre\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbnd\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meu\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjp\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgld\u001b[39m\u001b[38;5;124m\"\u001b[39m], cor_days_out, pred_distance, df, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msp\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mre\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbnd\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meu\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjp\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgld\u001b[39m\u001b[38;5;124m\"\u001b[39m], div_data)\n\u001b[0;32m----> 4\u001b[0m add_pred_differences(pred_distance, baseline_asset, assets, df)\n",
      "Cell \u001b[0;32mIn[49], line 5\u001b[0m, in \u001b[0;36madd_pred_differences\u001b[0;34m(pred_distance, baseline_asset, assets, df)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx, asset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(assets):\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#       for idx2, asset2 in enumerate(assets[idx + 1: ]): if you want all differences\u001b[39;00m\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m asset \u001b[38;5;241m!=\u001b[39m baseline_asset:\n\u001b[0;32m----> 5\u001b[0m             change_asset \u001b[38;5;241m=\u001b[39m df[asset \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_fut_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(pred_distance) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdys\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      6\u001b[0m             change_baseline \u001b[38;5;241m=\u001b[39m df[baseline_asset \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_fut_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(pred_distance) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdys\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      7\u001b[0m             diff \u001b[38;5;241m=\u001b[39m change_asset \u001b[38;5;241m-\u001b[39m change_baseline \n",
      "File \u001b[0;32m/opt/miniconda3/envs/ProbStock/lib/python3.11/site-packages/pandas/core/frame.py:4090\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4088\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4089\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4090\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[1;32m   4091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4092\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ProbStock/lib/python3.11/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'mid_fut_10dys'"
     ]
    }
   ],
   "source": [
    "# add_correlaries(cor_assets, cor_days_out, pred_distance, df, assets)\n",
    "div_data = load_div_data([\"sp\", \"re\", \"bnd\", \"eu\", \"jp\", \"gld\"], [None, \"IYRDividend.csv\", None, None, None, None])\n",
    "add_correlaries_div(cor_assets, cor_days_out, pred_distance, df, assets, div_data)\n",
    "add_pred_differences(pred_distance, baseline_asset, assets, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfa994e-5cbd-4e5e-bbf0-d60e48ef2d75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab83581",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6e696b-568c-40b8-a068-7cd2cbf89b8c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb314bac-7252-40ae-af70-ecdbcef4a3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[8060: 8061, [0, 1, 2, 3, 4, 5, 6, 7, 20]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b3fc03-cda9-4a1e-b355-9559dbdcd670",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df[(df['Date'] >= '2009-01-01') & (df['Date'] <= '2009-12-30')]\n",
    "df_test = df_test.reset_index(drop=True)\n",
    "df_train = df[(df['Date'] < '2009-01-01')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e27e5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds, actuals = test_preds(aliases, baseline_asset, pred_distance, df_test, df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a354470e-9d90-4b02-bb13-300c7855f4c9",
   "metadata": {},
   "source": [
    "# import pickle \n",
    "\n",
    "with open('preds.pkl', 'wb') as f:\n",
    "    pickle.dump(preds, f)\n",
    "\n",
    "with open('actuals.pkl', 'wb') as f:\n",
    "    pickle.dump(actuals, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857c67ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"preds.pkl\",'rb') as f:\n",
    "    preds = pickle.load(f)\n",
    "    \n",
    "with open(\"actuals.pkl\",'rb') as f:\n",
    "    actuals = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035f4422",
   "metadata": {},
   "outputs": [],
   "source": [
    "sign_cor_neg = 0\n",
    "sign_cor_pos = 0\n",
    "incor_actual_neg = 0\n",
    "incor_actual_pos = 0\n",
    "total_diff = 0\n",
    "total_count = 0\n",
    "total_correct = 0\n",
    "\n",
    "for pred, actual in zip(preds['gld'], actuals['gld']):\n",
    "    if pred != None and not pd.isna(actual):\n",
    "        if pred < 0 and actual < 0:\n",
    "            sign_cor_neg += 1\n",
    "            total_correct += 1\n",
    "        elif pred > 0 and actual > 0:\n",
    "            sign_cor_pos += 1\n",
    "            total_correct += 1\n",
    "        elif actual < 0:\n",
    "            incor_actual_neg += 1\n",
    "        else:\n",
    "            incor_actual_pos +=1\n",
    "        total_count += 1\n",
    "        total_diff += abs(pred - actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d548df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"pred neg actual neg\")\n",
    "print(sign_cor_neg)\n",
    "print(\"pred pos actual pos\")\n",
    "print(sign_cor_pos)\n",
    "print(\"pred pos actual neg\")\n",
    "print(incor_actual_neg) \n",
    "print(\"pred neg actual pos\")\n",
    "print(incor_actual_pos )\n",
    "\n",
    "print(\"-----\")\n",
    "print(\"total neg\")\n",
    "print(sign_cor_neg + incor_actual_neg)\n",
    "print(\"total pos\")\n",
    "print(sign_cor_pos + incor_actual_pos)\n",
    "print(\"----\")\n",
    "print(\"ave diff\")\n",
    "print(total_diff/total_count)\n",
    "print(\"percent correct\")\n",
    "print(total_correct/total_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb11bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 1\n",
    "period_counts = 0\n",
    "\n",
    "count = 0\n",
    "\n",
    "test_asset = 're'\n",
    "offset = 2\n",
    "\n",
    "for date, pred, actual in zip(df_test[\"Date\"][offset:], preds[test_asset][offset:], actuals[test_asset][offset:]):\n",
    "    if pred != None and not pd.isna(actual):\n",
    "        if count == 10:\n",
    "            # if pred > .01:\n",
    "            if pred > 0:\n",
    "                total *= (1 + 10 * pred * actual)\n",
    "                print(\"long: \" + str(date) + \": \" + str(total))\n",
    "            # if pred < -.01:\n",
    "            if pred < 0:\n",
    "                total *= (1 + 10  * abs(pred) * -(actual))\n",
    "                print(\"shorted: \" + str(date) + \": \" + str(total))\n",
    "            period_counts += 1\n",
    "    #         else:\n",
    "    #             total *= (1 + actual)\n",
    "            \n",
    "            count = 0\n",
    "        count += 1\n",
    "        \n",
    "        \n",
    "print(test_asset)\n",
    "print(total)\n",
    "print(period_counts)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8df5fcda-d45d-459f-9c16-bd33b0ca734f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a543b5e8-aec8-454f-839d-895491f909e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ef69b8-cefd-4f3f-b633-63ee8f55e3a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703d1804-89b2-4bbc-90d7-952921840070",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a499f26f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
